name: FastSpeech2
sample_rate: 22050
n_mels: 80


defaults:
  - hifigan/model/generator: v1

dataset:
  filenames:
    - train
    - val
  batch_sizes:
    - 16
    - 16
  preprocessed_path: 'path'
  phone_mapping: 'phone_ids2.json'

model:
  add_pitch_predictor: True
  add_energy_predictor: True
  d_model: 256
  duration_coeff: 0.25
  sample_rate: ${sample_rate}
  mappings_filepath: ${mappings_file}
  vocoder: ${hifigan.model.generator}
  vocoder_pretrain_path: "/root/andrey_b/tmp/NeMo/examples/tts/nemo_experiments/HifiGan/ruslan_pretrained/checkpoints/HifiGan--val_loss=0.1104-epoch=2891-last.ckpt"
  train_ds:
    dataset:
      _target_: 'nemo.collections.tts.data.datalayers.FastSpeech2DatasetMing'
      filename: 'train_fastspeech_tacotron.txt'
      # preprocessed_path: '/root/data/TTS/ruslan/preprocessed_data_tacotron/RUSLAN'
      phone_mapping: '/root/data/TTS/clean_ruslan/preprocessed_data_tacotron/RUSLAN/phone_ids2.json'
    dataloader_params:
      drop_last: false
      shuffle: true
      batch_size: 4  # 32 is default # initialy was 64, ming trains with 16, hifi was trained with 32x8
      num_workers: 1

  validation_ds:
    dataset:
      _target_: 'nemo.collections.tts.data.datalayers.FastSpeech2DatasetMing'
      filename: 'val_fastspeech_tacotron.txt'
      preprocessed_path: '/root/data/TTS/natasha/preprocessed_data_tacotron/natasha'
      phone_mapping: '/root/data/TTS/clean_ruslan/preprocessed_data_tacotron/RUSLAN/phone_ids2.json'
    dataloader_params:
      drop_last: false
      shuffle: false
      batch_size: 16
      num_workers: 4

  preprocessor:
    _target_: nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures
    dither: 0.0
    nfilt: ${n_mels}
    frame_splicing: 1
    highfreq: 8000
    log: true
    log_zero_guard_type: clamp
    log_zero_guard_value: 1e-05
    lowfreq: 0
    mag_power: 1.0
    n_fft: 1024
    n_window_size: 1024
    n_window_stride: 256
    normalize: null
    pad_to: 16
    pad_value: -11.52
    preemph: null
    sample_rate: ${sample_rate}
    window: hann

  # FFTransformer encoder
  encoder:
    _target_: nemo.collections.tts.modules.fastspeech2.FastSpeech2Encoder
    d_model: ${model.d_model}
    n_layers: 4
    n_attn_heads: 2
    d_attn_head: 256
    d_inner: 1024
    kernel_size: 9
    dropout: 0.1
    attn_dropout: 0.1
    n_embed: 56 # 56 # 360 # 74 # MING  # Should match number of tokens in symbol set +1 (pad_idx)
    padding_idx: 55 # 55 # 0 # 73 # MING

  # FFTransformer mel spec decoder
  decoder:
    _target_: nemo.collections.tts.modules.fastspeech2.MelSpecDecoder
    d_model: ${model.d_model}
    d_out: ${n_mels}
    n_layers: 6
    n_attn_heads: 2
    d_attn_head: 256
    d_inner: 1024
    kernel_size: 9
    dropout: 0.1
    attn_dropout: 0.1

  # VarianceAdaptor
  variance_adaptor:
    _target_: nemo.collections.tts.modules.fastspeech2.VarianceAdaptor
    d_model: ${model.d_model}
    dropout: 0.2
    dur_d_hidden: 256
    dur_kernel_size: 3
    pitch: ${model.add_pitch_predictor}
    log_pitch: False # MING
    n_f0_bins: 256
    pitch_kernel_size: 3
    pitch_min: -3.3745414368010813 # RUSLAN
    pitch_max: 10.64936926537594 # RUSLAN
    energy: ${model.add_energy_predictor}
    n_energy_bins: 256
    energy_kernel_size: 3
    energy_min: -1.5685399770736694 # RUSLAN
    energy_max: 5.1348490715026855 # RUSLAN

  optim:
    _target_: torch.optim.Adam
    betas: [0.9,0.98]
    lr: 1
    weight_decay: 1e-6
    # scheduler setup
  sched:
    name: NoamAnnealing
    warmup_steps: 4000
    min_lr: 1e-5
    d_model: ${model.d_model}

trainer:
  gpus: -1 # number of gpus
  max_epochs: 75
  num_nodes: 1
  accelerator: ddp
  accumulate_grad_batches: 1
  checkpoint_callback: False  # Provided by exp_manager
  logger: False  # Provided by exp_manager
  flush_logs_every_n_steps: 1000
  log_every_n_steps: 100
  check_val_every_n_epoch: 1

exp_manager:
  exp_dir: null
  name: ${name}
  create_tensorboard_logger: True
  create_checkpoint_callback: True
